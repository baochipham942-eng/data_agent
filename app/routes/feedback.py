"""
åé¦ˆä¸è¯„æµ‹ API è·¯ç”±ã€‚

æä¾›ï¼š
- ç”¨æˆ·åé¦ˆæäº¤
- LLM è‡ªåŠ¨è¯„ä¼°
- åé¦ˆå­¦ä¹ è§¦å‘
- ä¼˜åŒ–æŠ¥å‘Šè·å–
"""

import json
import sqlite3
import logging
from typing import Optional, List
from dataclasses import asdict

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field

from app.config import LOGS_DB_PATH
from app.services.llm_judge import (
    LLMJudge,
    FeedbackLearner,
    AutoOptimizer,
    EvaluationResult,
)

logger = logging.getLogger(__name__)


# ============ è¯·æ±‚/å“åº”æ¨¡å‹ ============

class FeedbackSubmitRequest(BaseModel):
    """æäº¤ä¸“å®¶è¯„åˆ†è¯·æ±‚"""
    conversation_id: str
    rating: int = Field(..., ge=1, le=5, description="ä¸“å®¶è¯„åˆ† 1-5")
    comment: Optional[str] = Field(None, description="è¯„è®º")
    auto_learn: bool = Field(True, description="æ˜¯å¦è‡ªåŠ¨å­¦ä¹ ")


class UserVoteRequest(BaseModel):
    """ç”¨æˆ·ç‚¹èµ/ç‚¹è¸©è¯·æ±‚"""
    conversation_id: str
    vote: str = Field(..., description="ç”¨æˆ·è¯„ä»·: 'like' | 'dislike' | 'none'")


class FeedbackSubmitResponse(BaseModel):
    """æäº¤åé¦ˆå“åº”"""
    success: bool
    message: str
    learning_result: Optional[dict] = None


class LLMEvaluateRequest(BaseModel):
    """LLM è¯„ä¼°è¯·æ±‚"""
    conversation_id: str
    force: bool = Field(False, description="å¼ºåˆ¶é‡æ–°è¯„ä¼°")


class LLMEvaluateResponse(BaseModel):
    """LLM è¯„ä¼°å“åº”"""
    success: bool
    evaluation: Optional[dict] = None
    error: Optional[str] = None


class BatchEvaluateRequest(BaseModel):
    """æ‰¹é‡è¯„ä¼°è¯·æ±‚"""
    conversation_ids: Optional[List[str]] = Field(None, description="æŒ‡å®šä¼šè¯IDï¼Œä¸ºç©ºåˆ™è¯„ä¼°æœ€è¿‘çš„")
    limit: int = Field(10, ge=1, le=50, description="è¯„ä¼°æ•°é‡")
    skip_evaluated: bool = Field(True, description="è·³è¿‡å·²è¯„ä¼°çš„")


class OptimizationReportResponse(BaseModel):
    """ä¼˜åŒ–æŠ¥å‘Šå“åº”"""
    weakness_report: dict
    suggestions: List[str]
    total_feedbacks: int
    high_score_count: int
    low_score_count: int


# ============ æ•°æ®åº“æ“ä½œ ============

def _get_conn():
    """è·å–æ•°æ®åº“è¿æ¥"""
    return sqlite3.connect(str(LOGS_DB_PATH))


def _init_feedback_table():
    """åˆå§‹åŒ–åé¦ˆè¡¨"""
    conn = _get_conn()
    cur = conn.cursor()
    
    # åˆ›å»ºåé¦ˆè¡¨
    cur.execute("""
        CREATE TABLE IF NOT EXISTS conversation_feedback (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            conversation_id TEXT NOT NULL,
            user_vote TEXT,
            expert_rating INTEGER,
            user_comment TEXT,
            llm_evaluation TEXT,
            llm_overall_score REAL,
            learned BOOLEAN DEFAULT 0,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(conversation_id)
        )
    """)
    
    # å°è¯•æ·»åŠ æ–°å­—æ®µï¼ˆå…¼å®¹æ—§è¡¨ï¼‰
    try:
        cur.execute("ALTER TABLE conversation_feedback ADD COLUMN user_vote TEXT")
    except:
        pass  # å­—æ®µå·²å­˜åœ¨
    
    try:
        cur.execute("ALTER TABLE conversation_feedback ADD COLUMN expert_rating INTEGER")
    except:
        pass  # å­—æ®µå·²å­˜åœ¨
    
    # è¿ç§»æ—§æ•°æ®ï¼šuser_rating -> expert_rating
    try:
        cur.execute("""
            UPDATE conversation_feedback 
            SET expert_rating = user_rating 
            WHERE expert_rating IS NULL AND user_rating IS NOT NULL
        """)
    except:
        pass
    
    # åˆ›å»ºç´¢å¼•
    cur.execute("""
        CREATE INDEX IF NOT EXISTS idx_feedback_conversation_id 
        ON conversation_feedback(conversation_id)
    """)
    cur.execute("""
        CREATE INDEX IF NOT EXISTS idx_feedback_score 
        ON conversation_feedback(llm_overall_score)
    """)
    
    conn.commit()
    conn.close()


def _save_feedback(
    conversation_id: str,
    user_vote: Optional[str] = None,  # 'like' | 'dislike' | 'none'
    expert_rating: Optional[int] = None,
    user_comment: Optional[str] = None,
    llm_evaluation: Optional[EvaluationResult] = None,
    learned: bool = False,
):
    """ä¿å­˜åé¦ˆ"""
    conn = _get_conn()
    cur = conn.cursor()
    
    llm_eval_json = json.dumps(asdict(llm_evaluation)) if llm_evaluation else None
    llm_score = llm_evaluation.overall_score if llm_evaluation else None
    
    cur.execute("""
        INSERT INTO conversation_feedback 
        (conversation_id, user_vote, expert_rating, user_comment, llm_evaluation, llm_overall_score, learned)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        ON CONFLICT(conversation_id) DO UPDATE SET
            user_vote = COALESCE(excluded.user_vote, user_vote),
            expert_rating = COALESCE(excluded.expert_rating, expert_rating),
            user_comment = COALESCE(excluded.user_comment, user_comment),
            llm_evaluation = COALESCE(excluded.llm_evaluation, llm_evaluation),
            llm_overall_score = COALESCE(excluded.llm_overall_score, llm_overall_score),
            learned = excluded.learned OR learned
    """, (conversation_id, user_vote, expert_rating, user_comment, llm_eval_json, llm_score, learned))
    
    conn.commit()
    conn.close()


def _get_feedback(conversation_id: str) -> Optional[dict]:
    """è·å–åé¦ˆ"""
    conn = _get_conn()
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    
    cur.execute("""
        SELECT * FROM conversation_feedback WHERE conversation_id = ?
    """, (conversation_id,))
    
    row = cur.fetchone()
    conn.close()
    
    if not row:
        return None
    
    return dict(row)


def _get_conversation_data(conversation_id: str) -> Optional[dict]:
    """è·å–ä¼šè¯æ•°æ®"""
    conn = _get_conn()
    conn.row_factory = sqlite3.Row
    cur = conn.cursor()
    
    # è·å–ä¼šè¯ä¿¡æ¯
    cur.execute("SELECT * FROM conversation WHERE id = ?", (conversation_id,))
    conv = cur.fetchone()
    
    if not conv:
        conn.close()
        return None
    
    # è·å–æ¶ˆæ¯
    cur.execute("""
        SELECT role, content, created_at
        FROM conversation_message
        WHERE conversation_id = ?
        ORDER BY created_at
    """, (conversation_id,))
    
    messages = [dict(row) for row in cur.fetchall()]
    conn.close()
    
    return {
        "conversation": dict(conv),
        "messages": messages,
    }


def _get_feedback_stats() -> dict:
    """è·å–åé¦ˆç»Ÿè®¡"""
    conn = _get_conn()
    cur = conn.cursor()
    
    stats = {"total": 0, "high_score": 0, "low_score": 0}
    
    try:
        cur.execute("SELECT COUNT(*) FROM conversation_feedback")
        stats["total"] = cur.fetchone()[0]
        
        cur.execute("""
            SELECT COUNT(*) FROM conversation_feedback 
            WHERE (user_rating >= 4 OR llm_overall_score >= 4.0)
        """)
        stats["high_score"] = cur.fetchone()[0]
        
        cur.execute("""
            SELECT COUNT(*) FROM conversation_feedback 
            WHERE (user_rating <= 2 OR llm_overall_score <= 2.5)
        """)
        stats["low_score"] = cur.fetchone()[0]
    except:
        pass
    
    conn.close()
    return stats


# ============ è·¯ç”±åˆ›å»º ============

def create_feedback_router(
    agent_memory,
    llm_service,
    prompt_manager=None,
    rag_knowledge_base=None,  # RAG çŸ¥è¯†åº“å®ä¾‹ï¼ˆå¯é€‰ï¼‰
) -> APIRouter:
    """
    åˆ›å»ºåé¦ˆè·¯ç”±ã€‚
    
    Args:
        agent_memory: Agent Memory å®ä¾‹
        llm_service: LLM æœåŠ¡å®ä¾‹
        prompt_manager: Promptç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
        rag_knowledge_base: RAG çŸ¥è¯†åº“å®ä¾‹ï¼ˆå¯é€‰ï¼‰
    """
    router = APIRouter(prefix="/api/feedback", tags=["feedback"])
    
    # åˆå§‹åŒ–è¡¨
    _init_feedback_table()
    
    # åˆå§‹åŒ– RAG ç›¸å…³æœåŠ¡ï¼ˆå¦‚æœæä¾›äº† RAG çŸ¥è¯†åº“ï¼‰
    rag_learner = None
    if rag_knowledge_base:
        from app.services.rag_learner import RAGLearner
        rag_learner = RAGLearner(rag_knowledge_base)
        logger.info("RAG å­¦ä¹ å™¨å·²åˆå§‹åŒ–")
    
    # åˆå§‹åŒ–æœåŠ¡
    llm_judge = LLMJudge(llm_service, prompt_manager=prompt_manager)
    feedback_learner = FeedbackLearner(agent_memory, llm_judge, rag_learner=rag_learner)
    auto_optimizer = AutoOptimizer(agent_memory, llm_service)
    
    @router.get("/scores")
    async def get_all_scores():
        """
        è·å–æ‰€æœ‰ä¼šè¯çš„è¯„åˆ†ï¼ˆç”¨æˆ·æ‰“åˆ†ã€ä¸“å®¶æ‰“åˆ†ã€æ¨¡å‹æ‰“åˆ†ï¼‰ã€‚
        """
        conn = _get_conn()
        cur = conn.cursor()
        
        cur.execute("""
            SELECT 
                conversation_id,
                user_vote,
                expert_rating,
                llm_overall_score,
                llm_evaluation,
                created_at
            FROM conversation_feedback
        """)
        
        rows = cur.fetchall()
        conn.close()
        
        scores = []
        for row in rows:
            conv_id, user_vote, expert_rating, llm_score, llm_eval_json, created_at = row
            
            # è§£æç”¨æˆ·æŠ•ç¥¨ä¸ºåˆ†æ•°
            user_vote_score = None
            if user_vote == 'like':
                user_vote_score = 5.0
            elif user_vote == 'dislike':
                user_vote_score = 1.0
            
            # è§£æ LLM è¯„ä¼°è§£é‡Š - ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
            llm_explanation = ""
            if llm_eval_json:
                try:
                    llm_eval = json.loads(llm_eval_json)
                    # æ„å»ºè§£é‡Šæ–‡æœ¬
                    parts = []
                    if llm_eval.get("reasoning"):
                        parts.append(f"ğŸ“ è¯„ä¼°ç†ç”±ï¼š{llm_eval['reasoning']}")
                    if llm_eval.get("strengths"):
                        parts.append(f"âœ… ä¼˜ç‚¹ï¼š{'ï¼›'.join(llm_eval['strengths'])}")
                    if llm_eval.get("weaknesses"):
                        parts.append(f"âš ï¸ ä¸è¶³ï¼š{'ï¼›'.join(llm_eval['weaknesses'])}")
                    if llm_eval.get("suggestions"):
                        parts.append(f"ğŸ’¡ å»ºè®®ï¼š{'ï¼›'.join(llm_eval['suggestions'])}")
                    
                    llm_explanation = '\n\n'.join(parts) if parts else (
                        f"SQLæ­£ç¡®æ€§: {llm_eval.get('sql_correctness', '-')}/5, "
                        f"ç»“æœè§£è¯»: {llm_eval.get('result_interpretation', '-')}/5, "
                        f"å®Œæ•´æ€§: {llm_eval.get('answer_completeness', '-')}/5, "
                        f"æ¸…æ™°åº¦: {llm_eval.get('expression_clarity', '-')}/5"
                    )
                except:
                    pass
            
            scores.append({
                "conversation_id": conv_id,
                "user_vote": user_vote,
                "user_vote_score": user_vote_score,
                "expert_score": expert_rating,
                "llm_score": llm_score,
                "llm_explanation": llm_explanation,
                "evaluated_at": created_at,
            })
        
        return {"success": True, "scores": scores}
    
    @router.post("/{conversation_id}/expert-score")
    async def save_expert_score(conversation_id: str, data: dict):
        """
        ä¿å­˜ä¸“å®¶è¯„åˆ†ã€‚
        """
        score = data.get("score")
        if not score or not isinstance(score, (int, float)) or score < 1 or score > 5:
            raise HTTPException(status_code=400, detail="è¯„åˆ†å¿…é¡»åœ¨ 1-5 ä¹‹é—´")
        
        _save_feedback(
            conversation_id=conversation_id,
            expert_rating=int(score),
        )
        
        return {"success": True, "message": "ä¸“å®¶è¯„åˆ†å·²ä¿å­˜"}
    
    @router.post("/submit", response_model=FeedbackSubmitResponse)
    async def submit_feedback(request: FeedbackSubmitRequest):
        """
        æäº¤ä¸“å®¶è¯„åˆ†ã€‚
        
        - ä¿å­˜ä¸“å®¶è¯„åˆ†ï¼ˆ1-5æ˜Ÿï¼‰
        - å¦‚æœè¯„åˆ†é«˜ï¼Œè§¦å‘è‡ªåŠ¨å­¦ä¹ 
        """
        conv_data = _get_conversation_data(request.conversation_id)
        if not conv_data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        # æå–ç”¨æˆ·é—®é¢˜å’Œ AI å›ç­”
        user_question = ""
        ai_response = ""
        generated_sql = None
        
        for msg in conv_data["messages"]:
            if msg["role"] == "user" and not user_question:
                user_question = msg["content"]
            elif msg["role"] == "assistant":
                ai_response = msg["content"]
                # å°è¯•æå– SQL
                if "SELECT" in msg["content"].upper():
                    import re
                    sql_match = re.search(
                        r'(SELECT\s+.+?(?:;|$))', 
                        msg["content"], 
                        re.IGNORECASE | re.DOTALL
                    )
                    if sql_match:
                        generated_sql = sql_match.group(1)
        
        learning_result = None
        
        # è‡ªåŠ¨å­¦ä¹ 
        if request.auto_learn:
            learning_result = await feedback_learner.learn_from_feedback(
                conversation_id=request.conversation_id,
                user_question=user_question,
                generated_sql=generated_sql,
                ai_response=ai_response,
                expert_rating=request.rating,  # ä¸“å®¶è¯„åˆ†
                user_rating=None,  # ç”¨æˆ·è¯„åˆ†ï¼ˆè¿™é‡Œæ²¡æœ‰ï¼Œå› ä¸ºè¿™æ˜¯ä¸“å®¶è¯„åˆ†æ¥å£ï¼‰
            )
            
            # è®°å½•å¼±é¡¹
            if learning_result.get("action") == "analyzed_weakness":
                analysis = learning_result.get("details", {}).get("analysis", {})
                if analysis.get("category"):
                    auto_optimizer.record_weakness(analysis["category"])
        
        # ä¿å­˜åé¦ˆ
        _save_feedback(
            conversation_id=request.conversation_id,
            expert_rating=request.rating,
            user_comment=request.comment,
            learned=learning_result.get("learned", False) if learning_result else False,
        )
        
        return FeedbackSubmitResponse(
            success=True,
            message="ä¸“å®¶è¯„åˆ†å·²æäº¤",
            learning_result=learning_result,
        )
    
    @router.post("/vote")
    async def submit_user_vote(request: UserVoteRequest):
        """
        æäº¤ç”¨æˆ·ç‚¹èµ/ç‚¹è¸©ã€‚
        
        - vote: 'like' | 'dislike' | 'none'
        """
        if request.vote not in ['like', 'dislike', 'none']:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„è¯„ä»·ç±»å‹")
        
        # ä¿å­˜ç”¨æˆ·è¯„ä»·
        _save_feedback(
            conversation_id=request.conversation_id,
            user_vote=request.vote if request.vote != 'none' else None,
        )
        
        return {
            "success": True,
            "message": "ç”¨æˆ·è¯„ä»·å·²ä¿å­˜",
            "vote": request.vote,
        }
    
    @router.post("/{conversation_id}/vote")
    async def submit_user_vote_by_id(conversation_id: str, data: dict):
        """
        é€šè¿‡è·¯å¾„å‚æ•°æäº¤ç”¨æˆ·ç‚¹èµ/ç‚¹è¸©ã€‚
        """
        vote = data.get("vote", "")
        if vote not in ['like', 'dislike', 'none']:
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„è¯„ä»·ç±»å‹")
        
        # ä¿å­˜ç”¨æˆ·è¯„ä»·
        _save_feedback(
            conversation_id=conversation_id,
            user_vote=vote if vote != 'none' else None,
        )
        
        return {
            "success": True,
            "message": "ç”¨æˆ·è¯„ä»·å·²ä¿å­˜",
            "vote": vote,
        }
    
    @router.get("/{conversation_id}")
    async def get_feedback_by_id(conversation_id: str):
        """
        è·å–ä¼šè¯çš„åé¦ˆä¿¡æ¯ã€‚
        """
        feedback = _get_feedback(conversation_id)
        if not feedback:
            return {"exists": False}
        
        return {
            "exists": True,
            "feedback": {
                "user_vote": feedback.get("user_vote"),
                "expert_rating": feedback.get("expert_rating"),
                "llm_score": feedback.get("llm_overall_score"),
            }
        }
    
    @router.post("/evaluate", response_model=LLMEvaluateResponse)
    async def llm_evaluate(request: LLMEvaluateRequest):
        """
        ä½¿ç”¨ LLM è¯„ä¼°ä¼šè¯è´¨é‡ã€‚
        """
        # æ£€æŸ¥æ˜¯å¦å·²è¯„ä¼°
        if not request.force:
            existing = _get_feedback(request.conversation_id)
            if existing and existing.get("llm_evaluation"):
                return LLMEvaluateResponse(
                    success=True,
                    evaluation=json.loads(existing["llm_evaluation"]),
                )
        
        # è·å–ä¼šè¯æ•°æ®
        conv_data = _get_conversation_data(request.conversation_id)
        if not conv_data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        # æå–æ•°æ®
        user_question = ""
        ai_response = ""
        generated_sql = None
        sql_result = None
        
        for msg in conv_data["messages"]:
            if msg["role"] == "user" and not user_question:
                user_question = msg["content"]
            elif msg["role"] == "assistant":
                content = msg["content"]
                ai_response = content
                
                # æå– SQL å’Œç»“æœï¼ˆç®€åŒ–å¤„ç†ï¼‰
                if "SELECT" in content.upper():
                    import re
                    sql_match = re.search(
                        r'(SELECT\s+.+?(?:;|$))', 
                        content, 
                        re.IGNORECASE | re.DOTALL
                    )
                    if sql_match:
                        generated_sql = sql_match.group(1)
        
        try:
            # æ‰§è¡Œ LLM è¯„ä¼°
            evaluation = await llm_judge.evaluate(
                user_question=user_question,
                generated_sql=generated_sql,
                sql_result=sql_result,
                ai_response=ai_response,
            )
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            _save_feedback(
                conversation_id=request.conversation_id,
                llm_evaluation=evaluation,
            )
            
            # è·å–å·²æœ‰çš„ä¸“å®¶è¯„åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
            existing_feedback = _get_feedback(request.conversation_id)
            expert_rating = existing_feedback.get("expert_rating") if existing_feedback else None
            
            # è§¦å‘å­¦ä¹ 
            learning_result = await feedback_learner.learn_from_feedback(
                conversation_id=request.conversation_id,
                user_question=user_question,
                generated_sql=generated_sql,
                ai_response=ai_response,
                expert_rating=expert_rating,  # ä¼ é€’ä¸“å®¶è¯„åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
                llm_evaluation=evaluation,
            )
            
            # è®°å½•å¼±é¡¹
            if learning_result.get("action") == "analyzed_weakness":
                analysis = learning_result.get("details", {}).get("analysis", {})
                if analysis.get("category"):
                    auto_optimizer.record_weakness(analysis["category"])
            
            return LLMEvaluateResponse(
                success=True,
                evaluation=asdict(evaluation),
            )
            
        except Exception as e:
            logger.error(f"LLM è¯„ä¼°å¤±è´¥: {e}")
            return LLMEvaluateResponse(
                success=False,
                error=str(e),
            )
    
    @router.post("/batch-evaluate")
    async def batch_evaluate(request: BatchEvaluateRequest):
        """
        æ‰¹é‡ LLM è¯„ä¼°ã€‚
        """
        conn = _get_conn()
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()
        
        # è·å–å¾…è¯„ä¼°çš„ä¼šè¯
        if request.conversation_ids:
            placeholders = ",".join(["?" for _ in request.conversation_ids])
            cur.execute(f"""
                SELECT id FROM conversation 
                WHERE id IN ({placeholders})
                ORDER BY started_at DESC
            """, request.conversation_ids)
        else:
            if request.skip_evaluated:
                cur.execute("""
                    SELECT c.id FROM conversation c
                    LEFT JOIN conversation_feedback f ON c.id = f.conversation_id
                    WHERE f.llm_evaluation IS NULL
                    ORDER BY c.started_at DESC
                    LIMIT ?
                """, (request.limit,))
            else:
                cur.execute("""
                    SELECT id FROM conversation
                    ORDER BY started_at DESC
                    LIMIT ?
                """, (request.limit,))
        
        conversation_ids = [row["id"] for row in cur.fetchall()]
        conn.close()
        
        results = []
        for conv_id in conversation_ids:
            try:
                result = await llm_evaluate(LLMEvaluateRequest(
                    conversation_id=conv_id,
                    force=not request.skip_evaluated,
                ))
                results.append({
                    "conversation_id": conv_id,
                    "success": result.success,
                    "score": result.evaluation.get("overall_score") if result.evaluation else None,
                })
            except Exception as e:
                results.append({
                    "conversation_id": conv_id,
                    "success": False,
                    "error": str(e),
                })
        
        return {
            "total": len(conversation_ids),
            "results": results,
        }
    
    @router.get("/optimization-report", response_model=OptimizationReportResponse)
    async def get_optimization_report():
        """
        è·å–ä¼˜åŒ–æŠ¥å‘Šã€‚
        """
        stats = _get_feedback_stats()
        
        return OptimizationReportResponse(
            weakness_report=auto_optimizer.get_weakness_report(),
            suggestions=auto_optimizer.suggest_prompt_improvements(),
            total_feedbacks=stats["total"],
            high_score_count=stats["high_score"],
            low_score_count=stats["low_score"],
        )
    
    @router.get("/fewshot-examples/{category}")
    async def get_fewshot_examples(category: str, limit: int = 3):
        """
        è·å–æŒ‡å®šç±»åˆ«çš„ Few-shot ç¤ºä¾‹ã€‚
        """
        examples = await auto_optimizer.generate_fewshot_examples(category, limit)
        return {
            "category": category,
            "examples": examples,
        }
    
    @router.get("/{conversation_id}")
    async def get_feedback(conversation_id: str):
        """
        è·å–ä¼šè¯çš„åé¦ˆä¿¡æ¯ã€‚
        """
        feedback = _get_feedback(conversation_id)
        if not feedback:
            return {"exists": False}
        
        # è§£æ LLM è¯„ä¼°
        if feedback.get("llm_evaluation"):
            feedback["llm_evaluation"] = json.loads(feedback["llm_evaluation"])
        
        return {
            "exists": True,
            "feedback": feedback,
        }
    
    return router

