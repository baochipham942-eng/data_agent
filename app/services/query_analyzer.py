"""
æŸ¥è¯¢åˆ†ææœåŠ¡ã€‚

æä¾›ï¼š
- é—®é¢˜æ”¹å†™ï¼šå°†ç”¨æˆ·é—®é¢˜é‡æ–°è¡¨è¿°å¾—æ›´æ¸…æ™°
- è¡¨é€‰å–ï¼šåˆ†æéœ€è¦ä½¿ç”¨çš„æ•°æ®è¡¨
- ä¸šåŠ¡çŸ¥è¯†æ£€ç´¢ï¼šä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³è§„åˆ™
"""

import json
import re
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import logging

logger = logging.getLogger(__name__)


class QueryAnalyzer:
    """æŸ¥è¯¢åˆ†æå™¨"""
    
    def __init__(
        self,
        data_db_path: Path,
        knowledge_db_path: Optional[Path] = None,
        llm_service = None,  # æ–°å¢ï¼šLLM æœåŠ¡ï¼Œç”¨äºæ™ºèƒ½è¡¨é€‰æ‹©
        prompt_manager = None,  # æ–°å¢ï¼šPromptç®¡ç†å™¨
    ):
        """
        åˆå§‹åŒ–æŸ¥è¯¢åˆ†æå™¨ã€‚
        
        Args:
            data_db_path: æ•°æ®æ•°æ®åº“è·¯å¾„
            knowledge_db_path: ä¸šåŠ¡çŸ¥è¯†åº“è·¯å¾„
            llm_service: LLM æœåŠ¡å®ä¾‹ï¼Œç”¨äºæ™ºèƒ½è¡¨é€‰æ‹©
            prompt_manager: Promptç®¡ç†å™¨ï¼Œç”¨äºè·å–æ¿€æ´»çš„prompt
        """
        self.data_db_path = Path(data_db_path)
        self.knowledge_db_path = Path(knowledge_db_path) if knowledge_db_path else None
        self.llm = llm_service
        self.prompt_manager = prompt_manager
        
        # ç¼“å­˜è¡¨ç»“æ„ä¿¡æ¯
        self._table_info_cache: Dict[str, Dict[str, Any]] = {}
        self._schema_description: str = ""  # ç¼“å­˜ schema æè¿°
        self._load_table_info()
        
        # åˆ†æç»“æœç¼“å­˜ï¼ˆé¿å…é‡å¤åˆ†æç›¸åŒé—®é¢˜ï¼‰
        self._analysis_cache: Dict[str, Dict[str, Any]] = {}
        self._cache_max_size = 100  # æœ€å¤šç¼“å­˜100ä¸ªåˆ†æç»“æœ
    
    def _get_table_select_prompt(self) -> str:
        """è·å–è¡¨é€‰æ‹© Prompt"""
        default_prompt = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œä»ä»¥ä¸‹æ•°æ®åº“è¡¨ä¸­é€‰æ‹©æœ€ç›¸å…³çš„è¡¨ã€‚

## æ•°æ®åº“è¡¨ç»“æ„
{schema_description}

## ç”¨æˆ·é—®é¢˜
{question}

## ä»»åŠ¡
è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„æ•°æ®è¡¨ï¼ˆå¯ä»¥é€‰å¤šä¸ªï¼‰ã€‚

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{"tables": ["è¡¨å1", "è¡¨å2"], "reason": "é€‰æ‹©åŸå› "}}

åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚å¦‚æœæ²¡æœ‰ç›¸å…³è¡¨ï¼Œè¿”å› {{"tables": [], "reason": "åŸå› "}}"""
        
        if self.prompt_manager:
            return self.prompt_manager.get_active_prompt_content(
                "table_select_prompt",
                fallback=default_prompt
            )
        return default_prompt
    
    def _get_data_conn(self) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(str(self.data_db_path))
        conn.row_factory = sqlite3.Row
        return conn
    
    def _get_knowledge_conn(self) -> Optional[sqlite3.Connection]:
        """è·å–çŸ¥è¯†åº“è¿æ¥"""
        if not self.knowledge_db_path or not self.knowledge_db_path.exists():
            return None
        conn = sqlite3.connect(str(self.knowledge_db_path))
        conn.row_factory = sqlite3.Row
        return conn
    
    def _load_table_info(self) -> None:
        """åŠ è½½è¡¨ç»“æ„ä¿¡æ¯åˆ°ç¼“å­˜ï¼Œå¹¶ç”Ÿæˆ schema æè¿°ä¾› LLM ä½¿ç”¨"""
        try:
            conn = self._get_data_conn()
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰è¡¨å
            cursor.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name NOT LIKE 'sqlite_%'
            """)
            tables = [row[0] for row in cursor.fetchall()]
            
            schema_parts = []
            
            for table in tables:
                cursor.execute(f'PRAGMA table_info("{table}")')
                columns = []
                for col in cursor.fetchall():
                    columns.append({
                        "name": col["name"],
                        "type": col["type"],
                    })
                
                # è·å–è¡Œæ•°
                cursor.execute(f'SELECT COUNT(*) FROM "{table}"')
                row_count = cursor.fetchone()[0]
                
                self._table_info_cache[table] = {
                    "name": table,
                    "columns": columns,
                    "column_names": [c["name"] for c in columns],
                    "row_count": row_count,
                }
                
                # ç”Ÿæˆè¯¥è¡¨çš„ schema æè¿°
                col_desc = ", ".join([f"{c['name']}({c['type']})" for c in columns[:10]])
                if len(columns) > 10:
                    col_desc += f" ... ç­‰å…± {len(columns)} ä¸ªå­—æ®µ"
                schema_parts.append(f"- {table} ({row_count}è¡Œ): {col_desc}")
            
            # ç¼“å­˜å®Œæ•´çš„ schema æè¿°
            self._schema_description = "\n".join(schema_parts)
            
            conn.close()
            logger.info(f"åŠ è½½äº† {len(self._table_info_cache)} ä¸ªè¡¨çš„ç»“æ„ä¿¡æ¯")
        except Exception as e:
            logger.error(f"åŠ è½½è¡¨ç»“æ„ä¿¡æ¯å¤±è´¥: {e}")
    
    def get_table_info(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰è¡¨ä¿¡æ¯"""
        return self._table_info_cache
    
    def analyze_tables(self, question: str) -> List[Dict[str, Any]]:
        """
        åˆ†æé—®é¢˜å¯èƒ½æ¶‰åŠçš„è¡¨ã€‚
        
        åŸºäºå…³é”®è¯åŒ¹é…å’Œè¡¨å/åˆ—åç›¸ä¼¼åº¦ã€‚
        """
        question_lower = question.lower()
        matched_tables = []
        
        # å¸¸è§ä¸šåŠ¡è¯æ±‡åˆ°è¡¨åçš„æ˜ å°„
        keyword_table_map = {
            # é”€å”®ç›¸å…³
            "é”€å”®": ["sales", "orders", "order", "transactions"],
            "é”€é‡": ["sales", "orders", "order", "transactions"],
            "è®¢å•": ["orders", "order", "sales"],
            "äº¤æ˜“": ["transactions", "orders", "sales"],
            "æ”¶å…¥": ["sales", "revenue", "orders"],
            "è¥æ”¶": ["sales", "revenue", "orders"],
            "é‡‘é¢": ["sales", "orders", "transactions"],
            
            # è®¿é—®/äº‹ä»¶ç›¸å…³ - é‡è¦ï¼
            "è®¿é—®": ["gio_event", "events", "page_view", "visits"],
            "è®¿é—®é‡": ["gio_event", "events", "page_view", "visits"],
            "æµè§ˆ": ["gio_event", "events", "page_view"],
            "ç‚¹å‡»": ["gio_event", "events", "clicks"],
            "äº‹ä»¶": ["gio_event", "events", "event_dic"],
            "é¡µé¢": ["gio_event", "page_dic", "pages"],
            "PV": ["gio_event", "page_view"],
            "UV": ["gio_event", "visitors"],
            "DAU": ["gio_event", "users", "active_users"],
            "MAU": ["gio_event", "users", "active_users"],
            "æ´»è·ƒ": ["gio_event", "users", "active_users"],
            "æ—¥æ´»": ["gio_event", "users"],
            "æœˆæ´»": ["gio_event", "users"],
            "app": ["gio_event", "apps", "applications"],
            "APP": ["gio_event", "apps", "applications"],
            "MPA": ["gio_event"],  # ä¼ä¸šè¯æ±‡ä¹Ÿæ·»åŠ æ˜ å°„
            
            # æ¸ é“/æ¥æºç›¸å…³
            "æ¸ é“": ["gio_event", "channels", "sources", "data_source"],
            "æ¥æº": ["gio_event", "data_source", "sources"],
            "çœä»½": ["gio_event", "regions", "locations"],
            
            # ç»é”€å•†/é—¨åº—ç›¸å…³
            "ç»é”€å•†": ["dealer_store_info", "dealers"],
            "é—¨åº—": ["dealer_store_info", "stores", "shops"],
            "åº—é“º": ["dealer_store_info", "stores"],
            
            # äº§å“ç›¸å…³
            "äº§å“": ["products", "product", "items", "goods"],
            "å•†å“": ["products", "product", "items", "goods"],
            "è´§å“": ["products", "product", "items", "goods"],
            
            # å®¢æˆ·ç›¸å…³
            "å®¢æˆ·": ["customers", "customer", "users", "clients"],
            "ç”¨æˆ·": ["users", "customers", "customer", "gio_event"],
            "ä¼šå‘˜": ["members", "customers", "users"],
            
            # åŒºåŸŸç›¸å…³
            "åŒºåŸŸ": ["regions", "area", "locations", "gio_event"],
            "åœ°åŒº": ["regions", "area", "locations", "gio_event"],
            "åŸå¸‚": ["cities", "city", "locations"],
            
            # æ—¶é—´ç›¸å…³
            "æ—¥æœŸ": ["gio_event", "sales", "dates", "calendar"],
            "æ—¶é—´": ["gio_event", "sales", "dates", "calendar", "time"],
            "æŒ‰æ—¥": ["gio_event", "sales"],
            "æŒ‰å¤©": ["gio_event", "sales"],
            "æŒ‰æœˆ": ["gio_event", "sales"],
            
            # åº“å­˜ç›¸å…³
            "åº“å­˜": ["inventory", "stock"],
            "ä»“åº“": ["warehouse", "inventory"],
            
            # å‘˜å·¥ç›¸å…³
            "å‘˜å·¥": ["employees", "staff", "workers"],
            
            # ç»Ÿè®¡/åˆ†æç›¸å…³ - é€šç”¨åŒ¹é…
            "ç»Ÿè®¡": ["gio_event", "sales"],
            "è¶‹åŠ¿": ["gio_event", "sales"],
            "åˆ†æ": ["gio_event", "sales"],
        }
        
        # æ£€æŸ¥å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        for keyword, possible_tables in keyword_table_map.items():
            # åŒæ—¶æ£€æŸ¥åŸé—®é¢˜å’Œå°å†™ç‰ˆæœ¬ï¼Œæ”¯æŒå¤§å†™å…³é”®è¯å¦‚ DAU, MPA
            if keyword.lower() in question_lower or keyword in question:
                for table_pattern in possible_tables:
                    for table_name, table_info in self._table_info_cache.items():
                        if table_pattern in table_name.lower():
                            if table_name not in [t["name"] for t in matched_tables]:
                                matched_tables.append({
                                    "name": table_name,
                                    "columns": table_info["column_names"][:5],  # åªæ˜¾ç¤ºå‰5åˆ—
                                    "row_count": table_info["row_count"],
                                    "match_reason": f"åŒ…å«å…³é”®è¯ '{keyword}'",
                                })
        
        # æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦ç›´æ¥æåˆ°è¡¨å
        for table_name, table_info in self._table_info_cache.items():
            if table_name.lower() in question_lower:
                if table_name not in [t["name"] for t in matched_tables]:
                    matched_tables.append({
                        "name": table_name,
                        "columns": table_info["column_names"][:5],
                        "row_count": table_info["row_count"],
                        "match_reason": "é—®é¢˜ä¸­ç›´æ¥æåŠ",
                    })
        
        # æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦æåˆ°åˆ—å
        for table_name, table_info in self._table_info_cache.items():
            for col in table_info["column_names"]:
                if col.lower() in question_lower or col.replace("_", " ").lower() in question_lower:
                    if table_name not in [t["name"] for t in matched_tables]:
                        matched_tables.append({
                            "name": table_name,
                            "columns": table_info["column_names"][:5],
                            "row_count": table_info["row_count"],
                            "match_reason": f"åŒ…å«å­—æ®µ '{col}'",
                        })
                        break
        
        # ã€æ™ºèƒ½å›é€€ã€‘å¦‚æœå…³é”®è¯åŒ¹é…æ²¡æœ‰ç»“æœï¼Œä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½è¡¨é€‰æ‹©
        if not matched_tables and self.llm and self._schema_description:
            logger.info(f"å…³é”®è¯åŒ¹é…æ— ç»“æœï¼Œå¯ç”¨ LLM æ™ºèƒ½è¡¨é€‰æ‹©: {question}")
            llm_selected = self._llm_select_tables(question)
            if llm_selected:
                matched_tables = llm_selected
        
        return matched_tables[:5]  # æœ€å¤šè¿”å›5ä¸ªè¡¨
    
    def _llm_select_tables(self, question: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ LLM æ™ºèƒ½é€‰æ‹©ç›¸å…³çš„æ•°æ®è¡¨ã€‚
        
        å½“å…³é”®è¯åŒ¹é…å¤±è´¥æ—¶ï¼Œè®© LLM æ ¹æ® schema ç†è§£é—®é¢˜è¯­ä¹‰æ¥é€‰æ‹©è¡¨ã€‚
        
        ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ è¶…æ—¶å’Œé”™è¯¯å¤„ç†ï¼Œé¿å…LLMè°ƒç”¨é˜»å¡æ•´ä¸ªæœåŠ¡ã€‚
        """
        if not self.llm:
            return []
        
        prompt = self._get_table_select_prompt().format(
            schema_description=self._schema_description,
            question=question,
        )
        
        try:
            # ã€å…³é”®ä¿®å¤ã€‘åŒæ­¥è°ƒç”¨ LLMï¼Œä½†æ·»åŠ è¶…æ—¶å’Œé”™è¯¯å¤„ç†
            # æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•ä¼šåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆç”±APIè·¯ç”±å±‚å¤„ç†ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œä¿æŒåŒæ­¥å³å¯
            response = self.llm._client.chat.completions.create(
                model=self.llm.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=500,
                timeout=10.0,  # 10ç§’è¶…æ—¶
            )
            result_text = response.choices[0].message.content or ""
            
            # è§£æ JSON
            import re
            json_match = re.search(r'\{[\s\S]*\}', result_text)
            if json_match:
                result = json.loads(json_match.group())
                selected_tables = result.get("tables", [])
                reason = result.get("reason", "LLM æ™ºèƒ½é€‰æ‹©")
                
                matched = []
                for table_name in selected_tables:
                    if table_name in self._table_info_cache:
                        table_info = self._table_info_cache[table_name]
                        matched.append({
                            "name": table_name,
                            "columns": table_info["column_names"][:5],
                            "row_count": table_info["row_count"],
                            "match_reason": f"ğŸ¤– AIæ™ºèƒ½é€‰æ‹©: {reason}",
                        })
                
                if matched:
                    logger.info(f"LLM é€‰æ‹©äº†è¡¨: {[m['name'] for m in matched]}")
                return matched
                
        except Exception as e:
            # ã€å…³é”®ä¿®å¤ã€‘LLMè°ƒç”¨å¤±è´¥ä¸åº”è¯¥é˜»å¡æ•´ä¸ªæœåŠ¡ï¼Œåªè®°å½•é”™è¯¯å¹¶è¿”å›ç©ºåˆ—è¡¨
            logger.warning(f"LLM è¡¨é€‰æ‹©å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")
        
        return []
    
    def get_relevant_knowledge(self, question: str) -> List[Dict[str, Any]]:
        """
        ä»ä¸šåŠ¡çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³çŸ¥è¯†ã€‚
        
        Returns:
            åŒ…å« time_rules, terms, mappings çš„åˆ—è¡¨
        """
        knowledge_items = []
        
        if not self.knowledge_db_path:
            return knowledge_items
        
        conn = self._get_knowledge_conn()
        if not conn:
            return knowledge_items
        
        try:
            cursor = conn.cursor()
            question_lower = question.lower()
            
            # 1. æ£€ç´¢æ—¶é—´è§„åˆ™
            cursor.execute("SELECT * FROM time_rules ORDER BY priority DESC")
            time_rules = cursor.fetchall()
            
            for rule in time_rules:
                keyword = rule["keyword"]
                if keyword in question or keyword in question_lower:
                    try:
                        config = json.loads(rule["rule_config"])
                        # è®¡ç®—å®é™…æ—¶é—´èŒƒå›´
                        time_desc = self._compute_time_description(rule["rule_type"], config)
                        knowledge_items.append({
                            "type": "time_rule",
                            "keyword": keyword,
                            "description": rule["description"] or time_desc,
                            "value": time_desc,
                        })
                    except:
                        pass
            
            # 2. æ£€ç´¢ä¸šåŠ¡æœ¯è¯­
            cursor.execute("SELECT * FROM business_terms")
            terms = cursor.fetchall()
            
            for term in terms:
                term_name = term["term"]
                if term_name in question or term_name.lower() in question_lower:
                    knowledge_items.append({
                        "type": "term",
                        "keyword": term_name,
                        "description": term["definition"],
                        "value": term["sql_expression"] if term["sql_expression"] else None,
                    })
            
            # 3. æ£€ç´¢å­—æ®µæ˜ å°„
            cursor.execute("SELECT * FROM field_mappings")
            mappings = cursor.fetchall()
            
            for mapping in mappings:
                display_name = mapping["display_name"]
                if display_name in question or display_name.lower() in question_lower:
                    knowledge_items.append({
                        "type": "mapping",
                        "keyword": display_name,
                        "description": f"{mapping['table_name']}.{mapping['field_name']} = '{mapping['field_value']}'",
                        "value": mapping["field_value"],
                    })
            
            conn.close()
        except Exception as e:
            logger.error(f"æ£€ç´¢ä¸šåŠ¡çŸ¥è¯†å¤±è´¥: {e}")
            if conn:
                conn.close()
        
        return knowledge_items
    
    def _compute_time_description(self, rule_type: str, config: Dict) -> str:
        """è®¡ç®—æ—¶é—´è§„åˆ™çš„å®é™…æè¿°"""
        from datetime import timedelta
        
        now = datetime.now()
        
        if rule_type == "relative":
            days = config.get("days", 0)
            target_date = now + timedelta(days=days)
            return target_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        elif rule_type == "æœ€è¿‘Nå¤©":
            days = config.get("days", 7)
            start_date = now - timedelta(days=days - 1)
            return f"{start_date.strftime('%Y-%m-%d')} è‡³ {now.strftime('%Y-%m-%d')}"
        
        elif rule_type == "æœˆ":
            offset = config.get("offset", 0)
            year = now.year
            month = now.month + offset
            while month <= 0:
                month += 12
                year -= 1
            while month > 12:
                month -= 12
                year += 1
            return f"{year}å¹´{month}æœˆ"
        
        elif rule_type == "å­£åº¦":
            offset = config.get("offset", 0)
            current_quarter = (now.month - 1) // 3 + 1
            target_quarter = current_quarter + offset
            year = now.year
            while target_quarter <= 0:
                target_quarter += 4
                year -= 1
            while target_quarter > 4:
                target_quarter -= 4
                year += 1
            return f"{year}å¹´Q{target_quarter}"
        
        elif rule_type == "åŒç¯æ¯”":
            compare_type = config.get("type", "")
            type_desc = {
                "yoy": "ä¸å»å¹´åŒæœŸå¯¹æ¯”",
                "mom": "ä¸ä¸Šæœˆå¯¹æ¯”",
                "wow": "ä¸ä¸Šå‘¨å¯¹æ¯”",
            }
            return type_desc.get(compare_type, "å¯¹æ¯”åˆ†æ")
        
        return str(config)
    
    def rewrite_question(self, question: str, knowledge: List[Dict[str, Any]]) -> str:
        """
        æ ¹æ®ä¸šåŠ¡çŸ¥è¯†æ”¹å†™é—®é¢˜ï¼Œä½¿å…¶æ›´æ¸…æ™°ã€‚
        
        è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡æ¿æ›¿æ¢å®ç°ï¼Œå®é™…å¯ä»¥ç”¨ LLM æ¥åšæ›´æ™ºèƒ½çš„æ”¹å†™ã€‚
        """
        rewritten = question
        
        # æ›¿æ¢æ—¶é—´ç›¸å…³çš„è¯æ±‡
        for item in knowledge:
            if item["type"] == "time_rule":
                keyword = item["keyword"]
                value = item["value"]
                if keyword in rewritten and value:
                    # åœ¨é—®é¢˜åé¢è¿½åŠ å…·ä½“æ—¶é—´è¯´æ˜
                    pass  # ä¿æŒåŸé—®é¢˜ï¼Œå…·ä½“æ—¶é—´åœ¨çŸ¥è¯†ä¸­å±•ç¤º
        
        # å¦‚æœé—®é¢˜å¤ªçŸ­ï¼Œè¡¥å……ä¸€äº›ä¸Šä¸‹æ–‡
        if len(rewritten) < 10:
            rewritten = f"æŸ¥è¯¢{rewritten}çš„ç›¸å…³æ•°æ®"
        
        return rewritten
    
    def check_feasibility(self, question: str, tables: List[Dict[str, Any]], knowledge: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ£€æŸ¥é—®é¢˜æ˜¯å¦å¯ä»¥è¢«æ•°æ®åº“å›ç­”ã€‚
        
        Returns:
            {
                "can_answer": bool,
                "confidence": float,  # 0-1
                "reason": str,
                "suggestions": List[str],
            }
        """
        # æå–é—®é¢˜ä¸­çš„æ ¸å¿ƒéœ€æ±‚å…³é”®è¯
        question_lower = question.lower()
        
        # éœ€è¦æ•°æ®æ”¯æ’‘çš„æ ¸å¿ƒä¸šåŠ¡è¯
        business_keywords = [
            # é”€å”®ç›¸å…³
            "é”€é‡", "é”€å”®é¢", "é”€å”®", "æ”¶å…¥", "è¥æ”¶", "åˆ©æ¶¦", "æˆæœ¬", "é‡‘é¢",
            "è®¢å•", "äº¤æ˜“", "è´­ä¹°", "ä¸‹å•",
            # è®¿é—®/äº‹ä»¶ç›¸å…³
            "è®¿é—®", "è®¿é—®é‡", "æµè§ˆ", "ç‚¹å‡»", "äº‹ä»¶", "é¡µé¢",
            "PV", "UV", "DAU", "MAU",
            # äº§å“ç›¸å…³
            "äº§å“", "å•†å“", "è´§å“", "SKU",
            # å®¢æˆ·/ç”¨æˆ·ç›¸å…³
            "å®¢æˆ·", "ç”¨æˆ·", "ä¼šå‘˜", "é¡¾å®¢",
            # åº“å­˜ç›¸å…³
            "åº“å­˜", "ä»“å‚¨", "å‡ºå…¥åº“",
            # äººå‘˜ç›¸å…³
            "å‘˜å·¥", "ç»©æ•ˆ", "ææˆ",
            # åŒºåŸŸ/æ¸ é“ç›¸å…³
            "åŒºåŸŸ", "é—¨åº—", "æ¸ é“", "æ¥æº", "çœä»½", "åœ°åŒº",
            # ç»é”€å•†ç›¸å…³
            "ç»é”€å•†", "åº—é“º",
        ]
        
        # æ£€æµ‹é—®é¢˜ä¸­åŒ…å«å“ªäº›ä¸šåŠ¡å…³é”®è¯
        found_keywords = [kw for kw in business_keywords if kw in question_lower]
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.0
        reasons = []
        suggestions = []
        
        # 1. å¦‚æœæœ‰åŒ¹é…çš„è¡¨ï¼ŒåŸºç¡€ç½®ä¿¡åº¦ +0.5
        if tables:
            # æ£€æŸ¥åŒ¹é…çš„è¡¨æ˜¯å¦çœŸçš„ç›¸å…³ï¼ˆä¸æ˜¯"å€™é€‰è¡¨"ï¼‰
            real_matches = [t for t in tables if t.get("match_reason") != "å€™é€‰è¡¨"]
            if real_matches:
                confidence += 0.5
                reasons.append(f"æ‰¾åˆ° {len(real_matches)} ä¸ªç›¸å…³æ•°æ®è¡¨")
            else:
                reasons.append("æ²¡æœ‰æ‰¾åˆ°ä¸é—®é¢˜ç›´æ¥ç›¸å…³çš„æ•°æ®è¡¨")
                suggestions.append("å½“å‰æ•°æ®åº“å¯èƒ½ä¸åŒ…å«ç›¸å…³ä¸šåŠ¡æ•°æ®")
        else:
            reasons.append("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„æ•°æ®è¡¨")
            suggestions.append("è¯·æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰ç›¸å…³ä¸šåŠ¡è¡¨")
        
        # 2. å¦‚æœæœ‰åŒ¹é…çš„ä¸šåŠ¡çŸ¥è¯†ï¼Œç½®ä¿¡åº¦ +0.2
        if knowledge:
            confidence += 0.2
            reasons.append(f"å‚è€ƒäº† {len(knowledge)} æ¡ä¸šåŠ¡çŸ¥è¯†")
        
        # 3. æ£€æŸ¥é—®é¢˜ä¸­çš„å…³é”®è¯æ˜¯å¦èƒ½æ˜ å°„åˆ°è¡¨/å­—æ®µ
        if found_keywords:
            matched_count = 0
            unmatched_keywords = []
            
            for kw in found_keywords:
                # æ£€æŸ¥è¿™ä¸ªå…³é”®è¯æ˜¯å¦åœ¨æŸä¸ªè¡¨çš„åŒ¹é…åŸå› ä¸­
                kw_matched = False
                for table in tables:
                    if kw in table.get("match_reason", ""):
                        kw_matched = True
                        break
                
                if kw_matched:
                    matched_count += 1
                else:
                    unmatched_keywords.append(kw)
            
            if matched_count > 0:
                confidence += 0.3 * (matched_count / len(found_keywords))
            
            if unmatched_keywords:
                reasons.append(f"ä»¥ä¸‹å…³é”®è¯æœªæ‰¾åˆ°å¯¹åº”æ•°æ®: {', '.join(unmatched_keywords)}")
                suggestions.append(f"æ•°æ®åº“ä¸­å¯èƒ½ç¼ºå°‘ {', '.join(unmatched_keywords)} ç›¸å…³çš„è¡¨æˆ–å­—æ®µ")
        
        # åˆ¤æ–­æ˜¯å¦å¯ä»¥å›ç­”
        can_answer = confidence >= 0.3 and len(tables) > 0
        
        # å¦‚æœä¸èƒ½å›ç­”ï¼Œç”Ÿæˆå‹å¥½çš„æç¤º
        if not can_answer:
            if not tables:
                suggestions.insert(0, "å»ºè®®å…ˆäº†è§£æ•°æ®åº“ä¸­æœ‰å“ªäº›æ•°æ®è¡¨")
            suggestions.append("æ‚¨å¯ä»¥å°è¯•è¯¢é—®æ•°æ®åº“ä¸­ç°æœ‰çš„æ•°æ®ï¼Œå¦‚ï¼š'æ•°æ®åº“æœ‰å“ªäº›è¡¨ï¼Ÿ'")
        
        return {
            "can_answer": can_answer,
            "confidence": round(confidence, 2),
            "reason": "ï¼›".join(reasons) if reasons else "åˆ†æå®Œæˆ",
            "suggestions": suggestions,
        }
    
    def get_available_capabilities(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æ•°æ®åº“å¯ä»¥å›ç­”çš„é—®é¢˜ç±»å‹ã€‚
        
        ç”¨äºåœ¨æ— æ³•å›ç­”æ—¶æç¤ºç”¨æˆ·å¯ä»¥é—®ä»€ä¹ˆã€‚
        """
        capabilities = []
        
        # åˆ†ææ¯ä¸ªè¡¨èƒ½å›ç­”ä»€ä¹ˆé—®é¢˜
        for table_name, table_info in self._table_info_cache.items():
            columns = table_info["column_names"]
            table_desc = {
                "table": table_name,
                "can_query": [],
            }
            
            # ç®€å•çš„åˆ—ååˆ°èƒ½åŠ›çš„æ˜ å°„
            capability_patterns = {
                "æ—¶é—´åˆ†æ": ["date", "time", "created", "updated", "timestamp"],
                "æ•°é‡ç»Ÿè®¡": ["count", "quantity", "amount", "num"],
                "é‡‘é¢åˆ†æ": ["price", "cost", "revenue", "profit", "amount", "total"],
                "åˆ†ç±»ç»Ÿè®¡": ["type", "category", "status", "level"],
                "ç”¨æˆ·åˆ†æ": ["user", "customer", "member"],
                "åœ°åŒºåˆ†æ": ["region", "city", "area", "location", "country"],
            }
            
            for cap_name, patterns in capability_patterns.items():
                for col in columns:
                    if any(p in col.lower() for p in patterns):
                        if cap_name not in table_desc["can_query"]:
                            table_desc["can_query"].append(cap_name)
                        break
            
            if table_desc["can_query"]:
                capabilities.append(table_desc)
        
        return {
            "tables_count": len(self._table_info_cache),
            "capabilities": capabilities,
        }

    def semantic_tokenize(self, question: str) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰åˆ†è¯ï¼šå°†ç”¨æˆ·é—®é¢˜æ‹†è§£ä¸ºè¯­ä¹‰å—ï¼Œå¹¶æ ‡æ³¨æ¯ä¸ªå—çš„ç±»å‹ã€‚
        
        ç±»ä¼¼å–œé©¬æ‹‰é›…çš„"é—®æ•°è¯­ä¹‰æ‹†è§£"æ•ˆæœï¼š
        "æœ¬å‘¨å°è¯´é¢‘é“çš„ä¸“è¾‘DAUè¶‹åŠ¿å¦‚ä½•ï¼Ÿç¯æ¯”ï¼Ÿ"
        â†’
        [
            {"text": "æœ¬å‘¨", "type": "time_rule", "knowledge": {...}},
            {"text": "å°è¯´é¢‘é“", "type": "field_mapping", "knowledge": {...}},
            {"text": "ä¸“è¾‘", "type": "term", "knowledge": {...}},
            {"text": "DAUè¶‹åŠ¿å¦‚ä½•", "type": "chart_hint", "knowledge": {...}},
            {"text": "ç¯æ¯”", "type": "comparison", "knowledge": {...}},
        ]
        """
        tokens = []
        remaining_text = question
        matched_positions = []  # è®°å½•å·²åŒ¹é…çš„ä½ç½®ï¼Œé¿å…é‡å¤
        
        # è·å–æ‰€æœ‰çŸ¥è¯†é¡¹
        conn = self._get_knowledge_conn()
        time_rules = []
        business_terms = []
        field_mappings = []
        
        if conn:
            try:
                cursor = conn.cursor()
                
                # è·å–æ—¶é—´è§„åˆ™ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çš„ï¼‰
                cursor.execute("SELECT * FROM time_rules ORDER BY LENGTH(keyword) DESC")
                time_rules = [dict(row) for row in cursor.fetchall()]
                
                # è·å–ä¸šåŠ¡æœ¯è¯­
                cursor.execute("SELECT * FROM business_terms ORDER BY LENGTH(term) DESC")
                business_terms = [dict(row) for row in cursor.fetchall()]
                
                # è·å–å­—æ®µæ˜ å°„
                cursor.execute("SELECT * FROM field_mappings ORDER BY LENGTH(display_name) DESC")
                field_mappings = [dict(row) for row in cursor.fetchall()]
                
                conn.close()
            except Exception as e:
                logger.error(f"è·å–çŸ¥è¯†åº“æ•°æ®å¤±è´¥: {e}")
                if conn:
                    conn.close()
        
        # å›¾è¡¨ç±»å‹å…³é”®è¯ï¼ˆå¤åˆè¯ä¼˜å…ˆåŒ¹é…ï¼Œæ”¾åœ¨å‰é¢ï¼‰
        chart_keywords = {
            # å¤åˆè¯ï¼ˆä¼˜å…ˆåŒ¹é…ï¼‰
            "å˜åŒ–è¶‹åŠ¿": {"type": "line", "label": "æŠ˜çº¿å›¾"},
            "è¶‹åŠ¿å˜åŒ–": {"type": "line", "label": "æŠ˜çº¿å›¾"},
            "èµ°åŠ¿å˜åŒ–": {"type": "line", "label": "æŠ˜çº¿å›¾"},
            "è¶‹åŠ¿èµ°åŠ¿": {"type": "line", "label": "æŠ˜çº¿å›¾"},
            "åˆ†å¸ƒæƒ…å†µ": {"type": "pie", "label": "é¥¼å›¾"},
            "å æ¯”åˆ†å¸ƒ": {"type": "pie", "label": "é¥¼å›¾"},
            "åˆ†å¸ƒå æ¯”": {"type": "pie", "label": "é¥¼å›¾"},
            "æ’åå¯¹æ¯”": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            "å¯¹æ¯”æ’å": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            # å•ä¸ªè¯ï¼ˆååŒ¹é…ï¼‰
            "è¶‹åŠ¿": {"type": "line", "label": "æŠ˜çº¿å›¾"},
            "èµ°åŠ¿": {"type": "line", "label": "æŠ˜çº¿å›¾"},
            "å˜åŒ–": {"type": "line", "label": "æŠ˜çº¿å›¾"},
            "å¦‚ä½•": {"type": "line", "label": "è¶‹åŠ¿åˆ†æ"},
            "æ€ä¹ˆæ ·": {"type": "line", "label": "è¶‹åŠ¿åˆ†æ"},
            "æ€æ ·": {"type": "line", "label": "è¶‹åŠ¿åˆ†æ"},
            "å¯¹æ¯”": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            "æ¯”è¾ƒ": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            "æ’å": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            "æ’è¡Œ": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            "Top": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            "top": {"type": "bar", "label": "æŸ±çŠ¶å›¾"},
            "å æ¯”": {"type": "pie", "label": "é¥¼å›¾"},
            "åˆ†å¸ƒ": {"type": "pie", "label": "é¥¼å›¾"},
            "æ„æˆ": {"type": "pie", "label": "é¥¼å›¾"},
            "æ¯”ä¾‹": {"type": "pie", "label": "é¥¼å›¾"},
        }
        
        # æ—¶é—´ç›¸å…³å…³é”®è¯ï¼ˆè¡¥å……æ•°æ®åº“ä¸­æ²¡æœ‰çš„ï¼‰
        time_keywords = {
            "æœ€è¿‘": {"label": "è¿‘æœŸæ—¶é—´", "value": "recent"},
            "è¿‘æœŸ": {"label": "è¿‘æœŸæ—¶é—´", "value": "recent"},
            "è¿‡å»": {"label": "è¿‡å»æ—¶é—´", "value": "past"},
            "å†å²": {"label": "å†å²æ•°æ®", "value": "historical"},
        }
        
        # åŒç¯æ¯”å…³é”®è¯
        comparison_keywords = {
            "ç¯æ¯”": {"type": "mom", "label": "ä¸ä¸ŠæœŸå¯¹æ¯”"},
            "åŒæ¯”": {"type": "yoy", "label": "ä¸åŒæœŸå¯¹æ¯”"},
            "å‘¨ç¯æ¯”": {"type": "wow", "label": "ä¸ä¸Šå‘¨å¯¹æ¯”"},
            "æœˆç¯æ¯”": {"type": "mom", "label": "ä¸ä¸Šæœˆå¯¹æ¯”"},
            "å¹´åŒæ¯”": {"type": "yoy", "label": "ä¸å»å¹´åŒæœŸå¯¹æ¯”"},
        }
        
        # 1. åŒ¹é…æ—¶é—´è§„åˆ™
        for rule in time_rules:
            keyword = rule["keyword"]
            if keyword in question:
                start_idx = question.find(keyword)
                end_idx = start_idx + len(keyword)
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«å…¶ä»–tokenè¦†ç›–
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    try:
                        config = json.loads(rule["rule_config"])
                        time_desc = self._compute_time_description(rule["rule_type"], config)
                    except:
                        time_desc = rule["description"]
                    
                    tokens.append({
                        "text": keyword,
                        "type": "time_rule",
                        "type_label": "æ—¶é—´è¯­ä¹‰è§„åˆ™",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": rule["description"],
                            "value": time_desc,
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # 1.5 åŒ¹é…è¡¥å……çš„æ—¶é—´å…³é”®è¯ï¼ˆæ•°æ®åº“ä¸­æ²¡æœ‰çš„ï¼‰
        for keyword, info in time_keywords.items():
            if keyword in question:
                start_idx = question.find(keyword)
                end_idx = start_idx + len(keyword)
                
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": keyword,
                        "type": "time_rule",
                        "type_label": "æ—¶é—´è¯­ä¹‰è§„åˆ™",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": info["label"],
                            "value": info["value"],
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # 1.6 ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¤æ‚æ—¶é—´è¡¨è¾¾å¼ï¼ˆæ›´å…¨é¢çš„æ‹†åˆ†ï¼‰
        # æ—¶é—´è¡¨è¾¾å¼æ¨¡å¼ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çš„ï¼‰
        time_patterns = [
            (r"æœ€è¿‘\d+[å¤©å‘¨æœˆå¹´]", "æœ€è¿‘Nå¤©/å‘¨/æœˆ/å¹´", "time_rule"),
            (r"è¿‘\d+[å¤©å‘¨æœˆå¹´]", "è¿‘Nå¤©/å‘¨/æœˆ/å¹´", "time_rule"),
            (r"è¿‡å»\d+[å¤©å‘¨æœˆå¹´]", "è¿‡å»Nå¤©/å‘¨/æœˆ/å¹´", "time_rule"),
            (r"å‰\d+[å¤©å‘¨æœˆå¹´]", "å‰Nå¤©/å‘¨/æœˆ/å¹´", "time_rule"),
            (r"æœ€è¿‘\d+æ—¥", "æœ€è¿‘Næ—¥", "time_rule"),
            (r"è¿‘\d+æ—¥", "è¿‘Næ—¥", "time_rule"),
            (r"\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥å·]?", "å…·ä½“æ—¥æœŸ", "time_rule"),
            (r"\d{4}[-/å¹´]\d{1,2}[æœˆ]?", "å¹´æœˆ", "time_rule"),
            (r"ä»Š[å¤©æ—¥]", "ä»Šå¤©", "time_rule"),
            (r"æ˜¨[å¤©æ—¥]", "æ˜¨å¤©", "time_rule"),
            (r"å‰[å¤©æ—¥]", "å‰å¤©", "time_rule"),
            (r"æœ¬[å‘¨æœˆå­£å¹´]", "æœ¬å‘¨/æœˆ/å­£/å¹´", "time_rule"),
            (r"ä¸Š[å‘¨æœˆå­£å¹´]", "ä¸Šå‘¨/æœˆ/å­£/å¹´", "time_rule"),
            (r"å»[å¹´æœˆ]", "å»å¹´/æœˆ", "time_rule"),
        ]
        
        for pattern, label, token_type in time_patterns:
            matches = re.finditer(pattern, question)
            for match in matches:
                start_idx = match.start()
                end_idx = match.end()
                matched_text = match.group()
                
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": matched_text,
                        "type": token_type,
                        "type_label": "æ—¶é—´è¯­ä¹‰è§„åˆ™",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": label,
                            "value": matched_text,
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # 1.7 åŒ¹é…ç»Ÿè®¡æ¨¡å¼ï¼ˆ"æŒ‰...ç»Ÿè®¡"ã€"æŒ‰...åˆ†ç»„"ç­‰ï¼Œä»¥åŠè‹±æ–‡"by day"ã€"group by"ç­‰ï¼‰
        stat_patterns = [
            # ä¸­æ–‡æ¨¡å¼
            (r"æŒ‰(.+?)ç»Ÿè®¡", "æŒ‰ç»´åº¦ç»Ÿè®¡", "dimension"),
            (r"æŒ‰(.+?)åˆ†ç»„", "æŒ‰ç»´åº¦åˆ†ç»„", "dimension"),
            (r"æŒ‰(.+?)èšåˆ", "æŒ‰ç»´åº¦èšåˆ", "dimension"),
            (r"æŒ‰(.+?)æ±‡æ€»", "æŒ‰ç»´åº¦æ±‡æ€»", "dimension"),
            (r"æŒ‰(.+?)åˆ†ç±»", "æŒ‰ç»´åº¦åˆ†ç±»", "dimension"),
            # è‹±æ–‡æ¨¡å¼ - éœ€è¦æ›´ç²¾ç¡®çš„åŒ¹é…ï¼Œé¿å…è¯¯åŒ¹é…
            (r"\bgroup\s+by\s+(\w+)\b", "æŒ‰ç»´åº¦åˆ†ç»„", "dimension"),  # "group by day"
            (r"\bby\s+(day|date|month|year|week|hour|minute)\b", "æŒ‰ç»´åº¦åˆ†ç»„", "dimension"),  # "by day", "by date" ç­‰æ—¶é—´ç»´åº¦
        ]
        
        for pattern, label, token_type in stat_patterns:
            matches = re.finditer(pattern, question)
            for match in matches:
                # åŒ¹é…æ•´ä¸ª"æŒ‰...ç»Ÿè®¡"æ¨¡å¼
                full_match_start = match.start()
                full_match_end = match.end()
                full_text = match.group(0)  # æ•´ä¸ªåŒ¹é…ï¼Œå¦‚"æŒ‰æ—¥æœŸç»Ÿè®¡"
                dimension_text = match.group(1)  # ç»´åº¦éƒ¨åˆ†ï¼Œå¦‚"æ—¥æœŸ"
                
                if not self._is_position_matched(full_match_start, full_match_end, matched_positions):
                    # å…ˆæ ‡è®°æ•´ä¸ªæ¨¡å¼ï¼Œé¿å…è¢«å…¶ä»–è§„åˆ™è¦†ç›–
                    matched_positions.append((full_match_start, full_match_end))
                    
                    # å¦‚æœç»´åº¦éƒ¨åˆ†æ²¡æœ‰è¢«å…¶ä»–è§„åˆ™åŒ¹é…ï¼Œå•ç‹¬æ ‡è®°ç»´åº¦
                    dim_start = match.start(1)
                    dim_end = match.end(1)
                    if not self._is_position_matched(dim_start, dim_end, matched_positions):
                        tokens.append({
                            "text": dimension_text,
                            "type": "dimension",
                            "type_label": "åˆ†æç»´åº¦",
                            "start": dim_start,
                            "end": dim_end,
                            "knowledge": {
                                "description": f"{label}ï¼š{dimension_text}",
                                "value": dimension_text,
                            },
                        })
                        matched_positions.append((dim_start, dim_end))
        
        # 1.8 åŒ¹é…æ•°å­—+å•ä½çš„æ—¶é—´è¡¨è¾¾å¼ï¼ˆå¦‚"7å¤©"ã€"30å¤©"ï¼‰ï¼Œä½†æ’é™¤å·²ç»è¢«åŒ¹é…çš„
        number_time_pattern = r"(\d+)([å¤©æ—¥å‘¨æœˆå¹´])"
        matches = re.finditer(number_time_pattern, question)
        for match in matches:
            start_idx = match.start()
            end_idx = match.end()
            matched_text = match.group(0)  # å¦‚"7å¤©"
            number = match.group(1)  # å¦‚"7"
            unit = match.group(2)  # å¦‚"å¤©"
            
            # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰"æœ€è¿‘"ã€"è¿‘"ç­‰è¯ï¼ˆé¿å…é‡å¤åŒ¹é…ï¼‰
            prev_start = max(0, start_idx - 2)
            prev_text = question[prev_start:start_idx]
            if prev_text not in ["æœ€è¿‘", "è¿‘", "è¿‡å»", "å‰"]:
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": matched_text,
                        "type": "time_rule",
                        "type_label": "æ—¶é—´è¯­ä¹‰è§„åˆ™",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": f"{number}{unit}",
                            "value": matched_text,
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # 2. åŒ¹é…åŒç¯æ¯”å…³é”®è¯
        for keyword, info in comparison_keywords.items():
            if keyword in question:
                start_idx = question.find(keyword)
                end_idx = start_idx + len(keyword)
                
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": keyword,
                        "type": "comparison",
                        "type_label": "åŒç¯æ¯”è¯­ä¹‰è§„åˆ™",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": info["label"],
                            "value": info["type"],
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # 3. åŒ¹é…ä¸šåŠ¡æœ¯è¯­
        for term in business_terms:
            term_name = term["term"]
            if term_name in question:
                start_idx = question.find(term_name)
                end_idx = start_idx + len(term_name)
                
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": term_name,
                        "type": "term",
                        "type_label": "ä¼ä¸šè¯æ±‡çŸ¥è¯†",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": term["definition"],
                            "value": term.get("sql_expression"),
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # 4. åŒ¹é…å­—æ®µæ˜ å°„
        for mapping in field_mappings:
            display_name = mapping["display_name"]
            if display_name in question:
                start_idx = question.find(display_name)
                end_idx = start_idx + len(display_name)
                
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": display_name,
                        "type": "field_mapping",
                        "type_label": "å­—æ®µæšä¸¾çŸ¥è¯†",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": f"{mapping['table_name']}.{mapping['field_name']} = '{mapping['field_value']}'",
                            "value": mapping["field_value"],
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # 5. åŒ¹é…å›¾è¡¨ç±»å‹å…³é”®è¯ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çš„å¤åˆè¯ï¼‰
        # å…ˆæŒ‰é•¿åº¦é™åºæ’åºï¼Œç¡®ä¿å¤åˆè¯ä¼˜å…ˆåŒ¹é…
        sorted_chart_keywords = sorted(chart_keywords.items(), key=lambda x: len(x[0]), reverse=True)
        for keyword, info in sorted_chart_keywords:
            # ä½¿ç”¨ finditer æ‰¾åˆ°æ‰€æœ‰åŒ¹é…ä½ç½®ï¼Œé¿å…åªåŒ¹é…ç¬¬ä¸€ä¸ª
            start_idx = question.find(keyword)
            while start_idx >= 0:
                end_idx = start_idx + len(keyword)
                
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": keyword,
                        "type": "chart_hint",
                        "type_label": "è‡ªåŠ¨å›¾è¡¨å±•ç¤º",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": info["label"],
                            "value": info["type"],
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
                    break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±è·³å‡ºï¼Œé¿å…é‡å¤
                
                # ç»§ç»­æŸ¥æ‰¾ä¸‹ä¸€ä¸ªåŒ¹é…ä½ç½®
                start_idx = question.find(keyword, start_idx + 1)
        
        # 6. æ£€æµ‹æŒ‡æ ‡å…³é”®è¯ï¼ˆå¸¸è§æ•°æ®æŒ‡æ ‡ï¼‰- æ”¯æŒå¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
        metric_keywords = {
            "é”€é‡": "æ•°é‡æŒ‡æ ‡",
            "é”€å”®é¢": "é‡‘é¢æŒ‡æ ‡",
            "æ”¶å…¥": "é‡‘é¢æŒ‡æ ‡",
            "è¥æ”¶": "é‡‘é¢æŒ‡æ ‡",
            "åˆ©æ¶¦": "é‡‘é¢æŒ‡æ ‡",
            "é‡‘é¢": "é‡‘é¢æŒ‡æ ‡",
            "è®¢å•": "æ•°é‡æŒ‡æ ‡",
            "è®¢å•æ•°": "æ•°é‡æŒ‡æ ‡",
            "ç”¨æˆ·æ•°": "æ•°é‡æŒ‡æ ‡",
            "è®¿é—®é‡": "è®¿é—®æ¬¡æ•°",
            "æµè§ˆé‡": "æµè§ˆæ¬¡æ•°",
            "ç‚¹å‡»é‡": "ç‚¹å‡»æ¬¡æ•°",
            "dau": "æ—¥æ´»è·ƒç”¨æˆ·",
            "mau": "æœˆæ´»è·ƒç”¨æˆ·",
            "uv": "ç‹¬ç«‹è®¿å®¢",
            "pv": "é¡µé¢æµè§ˆé‡",
            "gmv": "æˆäº¤æ€»é¢",
            "è½¬åŒ–ç‡": "æ¯”ç‡æŒ‡æ ‡",
            "ç‚¹å‡»ç‡": "æ¯”ç‡æŒ‡æ ‡",
            "è·³å‡ºç‡": "æ¯”ç‡æŒ‡æ ‡",
            "æ—¥æ´»": "æ—¥æ´»è·ƒç”¨æˆ·",
            "æœˆæ´»": "æœˆæ´»è·ƒç”¨æˆ·",
        }
        
        question_lower = question.lower()
        for keyword, desc in metric_keywords.items():
            keyword_lower = keyword.lower()
            # ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…
            if keyword_lower in question_lower:
                # æ‰¾åˆ°åŸé—®é¢˜ä¸­çš„å®é™…ä½ç½®ï¼ˆä¿æŒåŸå§‹å¤§å°å†™ï¼‰
                idx = question_lower.find(keyword_lower)
                if idx >= 0:
                    start_idx = idx
                    end_idx = start_idx + len(keyword)
                    original_text = question[start_idx:end_idx]  # ä¿ç•™åŸå§‹å¤§å°å†™
                    
                    if not self._is_position_matched(start_idx, end_idx, matched_positions):
                        tokens.append({
                            "text": original_text,
                            "type": "metric",
                            "type_label": "æŒ‡æ ‡",
                            "start": start_idx,
                            "end": end_idx,
                            "knowledge": {
                                "description": desc,
                                "value": keyword.upper() if keyword.isascii() else keyword,
                            },
                        })
                        matched_positions.append((start_idx, end_idx))
        
        # 7. æ£€æµ‹æ’åºè¯­ä¹‰å…³é”®è¯ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çš„å¤åˆè¯ï¼‰- æ”¾åœ¨ç»´åº¦ä¹‹å‰ï¼Œé¿å…è¢«è¦†ç›–
        sort_keywords = {
            "æœ€é«˜çš„": {"type": "desc", "label": "é™åºæ’åº"},
            "æœ€é«˜": {"type": "desc", "label": "é™åºæ’åº"},
            "æœ€å¤§çš„": {"type": "desc", "label": "é™åºæ’åº"},
            "æœ€å¤§": {"type": "desc", "label": "é™åºæ’åº"},
            "æœ€å¤šçš„": {"type": "desc", "label": "é™åºæ’åº"},
            "æœ€å¤š": {"type": "desc", "label": "é™åºæ’åº"},
            "æœ€ä½çš„": {"type": "asc", "label": "å‡åºæ’åº"},
            "æœ€ä½": {"type": "asc", "label": "å‡åºæ’åº"},
            "æœ€å°çš„": {"type": "asc", "label": "å‡åºæ’åº"},
            "æœ€å°": {"type": "asc", "label": "å‡åºæ’åº"},
            "æœ€å°‘çš„": {"type": "asc", "label": "å‡åºæ’åº"},
            "æœ€å°‘": {"type": "asc", "label": "å‡åºæ’åº"},
            "æ’å": {"type": "desc", "label": "æ’åæ’åº"},
            "æ’è¡Œ": {"type": "desc", "label": "æ’åæ’åº"},
            "Top": {"type": "desc", "label": "Top Næ’åº"},
            "top": {"type": "desc", "label": "Top Næ’åº"},
            "å‰": {"type": "desc", "label": "å‰Nå"},
        }
        
        # æŒ‰é•¿åº¦é™åºæ’åºï¼Œç¡®ä¿å¤åˆè¯ä¼˜å…ˆåŒ¹é…
        sorted_sort_keywords = sorted(sort_keywords.items(), key=lambda x: len(x[0]), reverse=True)
        question_lower = question.lower()
        for keyword, info in sorted_sort_keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in question_lower:
                idx = question_lower.find(keyword_lower)
                if idx >= 0:
                    start_idx = idx
                    end_idx = start_idx + len(keyword)
                    original_text = question[start_idx:end_idx]  # ä¿ç•™åŸå§‹å¤§å°å†™
                    
                    if not self._is_position_matched(start_idx, end_idx, matched_positions):
                        tokens.append({
                            "text": original_text,
                            "type": "sort",
                            "type_label": "æ’åºè¯­ä¹‰",
                            "start": start_idx,
                            "end": end_idx,
                            "knowledge": {
                                "description": info["label"],
                                "value": info["type"],
                            },
                        })
                        matched_positions.append((start_idx, end_idx))
                        break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±è·³å‡ºï¼Œé¿å…é‡å¤
        
        # 8. æ£€æµ‹ç»´åº¦å…³é”®è¯ï¼ˆåˆ†æç»´åº¦ï¼‰- æ”¾åœ¨æ’åºå…³é”®è¯ä¹‹å
        dimension_keywords = {
            "æ¸ é“": "æµé‡æ¥æºç»´åº¦",
            "æ¥æº": "æµé‡æ¥æºç»´åº¦",
            "åŸå¸‚": "åœ°ç†ç»´åº¦",
            "åœ°åŒº": "åœ°ç†ç»´åº¦",
            "çœä»½": "åœ°ç†ç»´åº¦",
            "åŒºåŸŸ": "åœ°ç†ç»´åº¦",
            "ç»é”€å•†": "ä¸šåŠ¡å®ä½“ç»´åº¦",
            "é—¨åº—": "ä¸šåŠ¡å®ä½“ç»´åº¦",
            "åº—é“º": "ä¸šåŠ¡å®ä½“ç»´åº¦",
            "å“ç‰Œ": "äº§å“ç»´åº¦",
            "å“ç±»": "äº§å“ç»´åº¦",
            "å•†å“": "äº§å“ç»´åº¦",
            "äº§å“": "äº§å“ç»´åº¦",
            "ç”¨æˆ·": "ç”¨æˆ·ç»´åº¦",
            "å®¢æˆ·": "ç”¨æˆ·ç»´åº¦",
            "ä¼šå‘˜": "ç”¨æˆ·ç»´åº¦",
            "æ—¶é—´": "æ—¶é—´ç»´åº¦",
            "æ—¥æœŸ": "æ—¶é—´ç»´åº¦",
            "æœˆä»½": "æ—¶é—´ç»´åº¦",
            "å¹´ä»½": "æ—¶é—´ç»´åº¦",
            "å‘¨": "æ—¶é—´ç»´åº¦",
            "å­£åº¦": "æ—¶é—´ç»´åº¦",
            "é¡µé¢": "è¡Œä¸ºç»´åº¦",
            "äº‹ä»¶": "è¡Œä¸ºç»´åº¦",
            "è®¾å¤‡": "è®¾å¤‡ç»´åº¦",
            "å¹³å°": "å¹³å°ç»´åº¦",
        }
        
        for keyword, desc in dimension_keywords.items():
            if keyword in question:
                start_idx = question.find(keyword)
                end_idx = start_idx + len(keyword)
                
                if not self._is_position_matched(start_idx, end_idx, matched_positions):
                    tokens.append({
                        "text": keyword,
                        "type": "dimension",
                        "type_label": "åˆ†æç»´åº¦",
                        "start": start_idx,
                        "end": end_idx,
                        "knowledge": {
                            "description": desc,
                            "value": keyword,
                        },
                    })
                    matched_positions.append((start_idx, end_idx))
        
        # æŒ‰ä½ç½®æ’åº
        tokens.sort(key=lambda x: x["start"])
        
        return tokens
    
    def _is_position_matched(self, start: int, end: int, matched_positions: List[Tuple[int, int]]) -> bool:
        """æ£€æŸ¥ä½ç½®æ˜¯å¦å·²è¢«åŒ¹é…"""
        for ms, me in matched_positions:
            # å¦‚æœæœ‰é‡å 
            if not (end <= ms or start >= me):
                return True
        return False

    def analyze(self, question: str, use_cache: bool = True) -> Dict[str, Any]:
        """
        å®Œæ•´åˆ†æä¸€ä¸ªæŸ¥è¯¢é—®é¢˜ã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
        Returns:
            {
                "original_question": str,
                "rewritten_question": str,
                "selected_tables": List[Dict],
                "relevant_knowledge": List[Dict],
                "semantic_tokens": List[Dict],  # æ–°å¢ï¼šè¯­ä¹‰åˆ†è¯ç»“æœ
                "analysis_time": str,
                "feasibility": Dict,
            }
        """
        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            question_key = question.strip().lower()
            if question_key in self._analysis_cache:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ: {question[:50]}...")
                return self._analysis_cache[question_key]
        
        # 1. è¯­ä¹‰åˆ†è¯
        semantic_tokens = self.semantic_tokenize(question)
        
        # 2. æ£€ç´¢ç›¸å…³ä¸šåŠ¡çŸ¥è¯†
        knowledge = self.get_relevant_knowledge(question)
        
        # 3. åˆ†æå¯èƒ½æ¶‰åŠçš„è¡¨
        tables = self.analyze_tables(question)
        
        # 4. æ£€æŸ¥å¯è¡Œæ€§
        feasibility = self.check_feasibility(question, tables, knowledge)
        
        # 5. æ”¹å†™é—®é¢˜
        rewritten = self.rewrite_question(question, knowledge)
        
        result = {
            "original_question": question,
            "rewritten_question": rewritten,
            "selected_tables": tables,
            "relevant_knowledge": knowledge,
            "semantic_tokens": semantic_tokens,
            "feasibility": feasibility,
            "analysis_time": datetime.now().isoformat(),
        }
        
        # æ›´æ–°ç¼“å­˜
        if use_cache:
            question_key = question.strip().lower()
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆFIFOï¼‰
            if len(self._analysis_cache) >= self._cache_max_size:
                oldest_key = next(iter(self._analysis_cache))
                del self._analysis_cache[oldest_key]
            self._analysis_cache[question_key] = result
        
        return result
    
    def clear_cache(self):
        """æ¸…ç©ºåˆ†æç»“æœç¼“å­˜"""
        self._analysis_cache.clear()
        logger.info("å·²æ¸…ç©ºåˆ†æç»“æœç¼“å­˜")


# å…¨å±€å•ä¾‹
_query_analyzer: Optional[QueryAnalyzer] = None


def get_query_analyzer() -> Optional[QueryAnalyzer]:
    """è·å–æŸ¥è¯¢åˆ†æå™¨å•ä¾‹"""
    return _query_analyzer


def init_query_analyzer(
    data_db_path: Path,
    knowledge_db_path: Optional[Path] = None,
    llm_service = None,
    prompt_manager = None,
) -> QueryAnalyzer:
    """åˆå§‹åŒ–æŸ¥è¯¢åˆ†æå™¨"""
    global _query_analyzer
    _query_analyzer = QueryAnalyzer(
        data_db_path=data_db_path,
        knowledge_db_path=knowledge_db_path,
        llm_service=llm_service,
        prompt_manager=prompt_manager,
    )
    return _query_analyzer

